{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "11rUjwwA9zqv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbV8Ifzxsvh9",
        "outputId": "6f9b9ad1-054b-45bf-9123-c0f31f33b86a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/Colab Notebooks/cleaned_training_data.csv'"
      ],
      "metadata": {
        "id": "mHrYj-yXsvkh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "gexsJcbYtEA3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "PKT6LEJytFLL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "IrogZzL_Vzhe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(file_path,  header=0)"
      ],
      "metadata": {
        "id": "REP43TAiZOBn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Subsample the majority class\n",
        "# data_majority = data[data['bad_flag'] == 0]\n",
        "# data_minority = data[data['bad_flag'] == 1]\n",
        "\n",
        "# # Downsample majority class\n",
        "# data_majority_downsampled = data_majority.sample(1*len(data_minority), random_state=42)\n",
        "\n",
        "# data_balanced = pd.concat([data_majority_downsampled, data_minority])\n",
        "\n",
        "# # Shuffle the balanced dataset\n",
        "# data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Nf3u2zB4v4CM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = data['bad_flag'].astype(float)\n",
        "X = data.drop(columns=['bad_flag'])"
      ],
      "metadata": {
        "id": "lBkMZSGQwhj8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.10, random_state=1000\n",
        ")"
      ],
      "metadata": {
        "id": "pM3O-ebSzBT-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_t, X_val, y_t, y_val = train_test_split(\n",
        "    X_train, y_train, stratify=y_train, test_size=0.20, random_state=1000\n",
        ")"
      ],
      "metadata": {
        "id": "raaAFJ2vzMws"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "train_data = pd.concat([X_t, y_t], axis=1)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_class = train_data[train_data['bad_flag'] == 0]  # Replace 'target_column' with your target name\n",
        "minority_class = train_data[train_data['bad_flag'] == 1]\n",
        "\n",
        "# Determine the target size for the majority class\n",
        "# Set to equal size or twice the minority class size\n",
        "majority_target_size = len(minority_class) * 5  # Set to *1 for equal size\n",
        "\n",
        "# Subsample majority class\n",
        "majority_class_downsampled = resample(\n",
        "    majority_class,\n",
        "    replace=False,  # No replacement\n",
        "    n_samples=majority_target_size,  # New size\n",
        "    random_state=42  # Ensure reproducibility\n",
        ")\n",
        "\n",
        "# Combine the subsampled majority class with the minority class\n",
        "balanced_data = pd.concat([majority_class_downsampled, minority_class])\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split back into features and target\n",
        "X_train_balanced = balanced_data.drop(columns=['bad_flag'])\n",
        "y_train_balanced = balanced_data['bad_flag']"
      ],
      "metadata": {
        "id": "lGPkhFb5i9vq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
        "# X_train_smote, y_train_smote = smote.fit_resample(X_t, y_t)"
      ],
      "metadata": {
        "id": "O8nXb8ZBirnn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_smote, y_train_smote = X_train_balanced, y_train_balanced"
      ],
      "metadata": {
        "id": "Fy3btBrnJRsT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numer = ['term', 'int_rate', 'emp_length', 'annual_inc', 'percent_bc_gt_75',\n",
        "        'dti', 'inq_last_6mths', 'mths_since_recent_inq',\n",
        "       'total_bc_limit', 'tot_cur_bal', 'internal_score']\n",
        "dummy = X_train.columns.difference(numer)"
      ],
      "metadata": {
        "id": "wxxxO4ZgJ3XO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_num_scaled = scaler.fit_transform(X_train_smote[numer])\n",
        "X_val_num_scaled = scaler.transform(X_val[numer])"
      ],
      "metadata": {
        "id": "VYJSAUuYJ8X-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled1 = pd.DataFrame(X_train_num_scaled, columns=numer, index=X_train_smote.index)\n",
        "X_train_scaled = pd.concat([X_train_scaled1, X_train_smote[dummy]], axis=1)\n",
        "\n",
        "X_val_scaled1 = pd.DataFrame(X_val_num_scaled, columns=numer, index=X_val.index)\n",
        "X_val_scaled = pd.concat([X_val_scaled1, X_val[dummy]], axis=1)"
      ],
      "metadata": {
        "id": "AK2_9wEQKL3y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(y_train_smote.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKQic98avjby",
        "outputId": "e5ee09c3-f04a-43f2-9f74-78e436b8c516"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bad_flag\n",
            "0.0    47260\n",
            "1.0     9452\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_val.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4Y2z9JXmb4B",
        "outputId": "a7a726f0-7990-4e10-8322-ea1616bd5b71"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bad_flag\n",
            "0.0    31740\n",
            "1.0     2363\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_smote.values, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val_scaled.values, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "rRK60e4htFQ1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_val_tensor.size(), y_val_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXwh2YEFCY8L",
        "outputId": "76e0458b-fe84-489b-fa37-866dc28b789f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([34103, 28]) torch.Size([34103])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_tensor.shape, y_train_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfJDwl-DCI0C",
        "outputId": "1c576bd6-cf0e-4e7e-da4e-596c9b4ae2d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([66164, 28]) torch.Size([66164])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3,  output_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=0.2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        # self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc4 = nn.Linear(hidden_size3, output_size)\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        # x = self.dropout2(x)\n",
        "        x = self.fc4(x)\n",
        "        # x = self.relu4(x)\n",
        "        # x = self.fc5(x)\n",
        "        # x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Model configuration\n",
        "input_size = X_train_tensor.shape[1]\n",
        "print(input_size)\n",
        "hidden_size1 = 128  # Configurable\n",
        "hidden_size2 = 256\n",
        "hidden_size3 = 64\n",
        "output_size = 1\n",
        "\n",
        "# Instantiate the model\n",
        "model = NeuralNet(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)"
      ],
      "metadata": {
        "id": "-9XeZuojtFTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938aa1cc-277b-4851-c8f8-0177b5b842dd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # Xavier (Glorot) initialization for weights\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        # Initialize biases to zero\n",
        "        nn.init.zeros_(m.bias)"
      ],
      "metadata": {
        "id": "WmMvSJyKnR2-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCGN_oqvnT74",
        "outputId": "f723da98-ee36-4a5f-d38e-0859047d570a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (fc1): Linear(in_features=28, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (relu2): ReLU()\n",
              "  (dropout1): Dropout(p=0.2, inplace=False)\n",
              "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training loop\n",
        "epochs = 200  # Configurable\n",
        "batch_size = 32\n",
        "clip_value = 1.0\n",
        "# Loss function and optimizer\n",
        "# criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "pos_weight = torch.tensor([47260 / 9452], dtype=torch.float)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # Binary Cross-Entropy Loss\n",
        "# criterion = FocalLoss(alpha=0.25, gamma=2.0, reduction='mean')\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "J4vQsWslL1SL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51c1309-5998-45ab-9fe5-497cf8942706"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 10  # Number of epochs with no improvement to wait\n",
        "# best_val_loss = float(\"inf\")\n",
        "early_stop_counter = 0"
      ],
      "metadata": {
        "id": "KFj-f6bHnkCn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "best_threshold = 0.5\n",
        "best_f1_score = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for i in range(0, len(X_train_tensor), batch_size):\n",
        "        # Get mini-batch\n",
        "        X_batch = X_train_tensor[i:i+batch_size]\n",
        "        y_batch = y_train_tensor[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(X_batch).squeeze(dim = -1)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        total_loss += loss.item()\n",
        "        # print(\"Model output shape:\", outputs.shape)\n",
        "        # print(\"Target shape:\", y_batch.shape)\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        train_outputs = model(X_train_tensor).squeeze()\n",
        "        train_predictions = (train_outputs > 0.5).float()\n",
        "        train_accuracy = accuracy_score(y_train_tensor.numpy(), train_predictions.numpy())\n",
        "        train_f1 = f1_score(y_train_tensor.numpy(), train_predictions.numpy())\n",
        "        train_recall = recall_score(y_train_tensor.numpy(), train_predictions.numpy())\n",
        "        train_precision = precision_score(y_train_tensor.numpy(), train_predictions.numpy())\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor).squeeze()\n",
        "        val_loss = criterion(val_outputs, y_val_tensor)\n",
        "\n",
        "        # # Generate probabilities for validation\n",
        "        val_probabilities = torch.sigmoid(val_outputs).numpy()\n",
        "\n",
        "        # Search for the best threshold to maximize F1-score\n",
        "        thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "        for threshold in thresholds:\n",
        "            val_predictions = (val_probabilities > threshold).astype(int)\n",
        "            current_f1 = f1_score(y_val_tensor.numpy(), val_predictions)\n",
        "\n",
        "            if current_f1 > best_f1_score:\n",
        "                best_f1_score = current_f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        # Apply the best threshold for current evaluation\n",
        "        val_predictions = (val_probabilities > best_threshold).astype(int)\n",
        "        val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions)\n",
        "        val_f1 = f1_score(y_val_tensor.numpy(), val_predictions)\n",
        "        val_recall = recall_score(y_val_tensor.numpy(), val_predictions)\n",
        "        val_precision = precision_score(y_val_tensor.numpy(), val_predictions)\n",
        "    scheduler.step(val_f1)\n",
        "    # Log epoch results\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "          f\"Loss: {total_loss / len(X_train_tensor):.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "          f\"Train F1: {train_f1:.4f}, Train Recall: {train_recall:.4f}, Train Precision: {train_precision:.4f}, \"\n",
        "          f\"Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}, \"\n",
        "          f\"Val F1: {val_f1:.4f}, Val Recall: {val_recall:.4f}, Val Precision: {val_precision:.4f}, \"\n",
        "          f\"Best Threshold: {best_threshold:.2f}\")\n",
        "\n",
        "    if val_f1 >= best_f1_score:\n",
        "        best_f1_score = val_f1\n",
        "        torch.save(model.state_dict(), \"/content/drive/My Drive/Colab Notebooks/neural_net_model.pth\")\n",
        "        print(f\"Best model saved with F1-score: {val_f1:.4f}\")\n",
        "        early_stop_counter = 0  # Reset early stopping counter\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "    # print(val_f1, best_f1_score, early_stop_counter)\n",
        "    # Check early stopping condition\n",
        "    if early_stop_counter >= patience:\n",
        "        print(f\"Early stopping triggered after {epoch+1} epochs!\")\n",
        "        break\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"/content/drive/My Drive/Colab Notebooks/neural_net_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu2WiWOWKjty",
        "outputId": "0d9c1d6d-9dcd-4098-8638-b7a8e4d80984"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 0.0341, Train Accuracy: 0.7733, Train F1: 0.3032, Train Recall: 0.2959, Train Precision: 0.3109, Val Loss: 0.7911, Val Accuracy: 0.7578, Val F1: 0.2059, Val Recall: 0.4532, Val Precision: 0.1332, Best Threshold: 0.58\n",
            "Best model saved with F1-score: 0.2059\n",
            "Epoch 2/200, Loss: 0.0336, Train Accuracy: 0.7787, Train F1: 0.3096, Train Recall: 0.2977, Train Precision: 0.3225, Val Loss: 0.7904, Val Accuracy: 0.7807, Val F1: 0.2102, Val Recall: 0.4211, Val Precision: 0.1400, Best Threshold: 0.59\n",
            "Best model saved with F1-score: 0.2102\n",
            "Epoch 3/200, Loss: 0.0334, Train Accuracy: 0.7749, Train F1: 0.3268, Train Recall: 0.3279, Train Precision: 0.3258, Val Loss: 0.7847, Val Accuracy: 0.7774, Val F1: 0.2108, Val Recall: 0.4291, Val Precision: 0.1397, Best Threshold: 0.59\n",
            "Best model saved with F1-score: 0.2108\n",
            "Epoch 4/200, Loss: 0.0332, Train Accuracy: 0.7723, Train F1: 0.3389, Train Recall: 0.3502, Train Precision: 0.3283, Val Loss: 0.7884, Val Accuracy: 0.7872, Val F1: 0.2111, Val Recall: 0.4109, Val Precision: 0.1420, Best Threshold: 0.60\n",
            "Best model saved with F1-score: 0.2111\n",
            "Epoch 5/200, Loss: 0.0331, Train Accuracy: 0.7744, Train F1: 0.3458, Train Recall: 0.3577, Train Precision: 0.3347, Val Loss: 0.7851, Val Accuracy: 0.8168, Val F1: 0.2137, Val Recall: 0.3593, Val Precision: 0.1520, Best Threshold: 0.62\n",
            "Best model saved with F1-score: 0.2137\n",
            "Epoch 6/200, Loss: 0.0329, Train Accuracy: 0.7794, Train F1: 0.3454, Train Recall: 0.3491, Train Precision: 0.3418, Val Loss: 0.7773, Val Accuracy: 0.7803, Val F1: 0.2147, Val Recall: 0.4333, Val Precision: 0.1427, Best Threshold: 0.59\n",
            "Best model saved with F1-score: 0.2147\n",
            "Epoch 7/200, Loss: 0.0328, Train Accuracy: 0.7792, Train F1: 0.3511, Train Recall: 0.3584, Train Precision: 0.3441, Val Loss: 0.7765, Val Accuracy: 0.7786, Val F1: 0.2148, Val Recall: 0.4372, Val Precision: 0.1424, Best Threshold: 0.59\n",
            "Best model saved with F1-score: 0.2148\n",
            "Epoch 8/200, Loss: 0.0326, Train Accuracy: 0.7810, Train F1: 0.3542, Train Recall: 0.3605, Train Precision: 0.3482, Val Loss: 0.7696, Val Accuracy: 0.7818, Val F1: 0.2133, Val Recall: 0.4270, Val Precision: 0.1422, Best Threshold: 0.59\n",
            "Epoch 9/200, Loss: 0.0326, Train Accuracy: 0.7842, Train F1: 0.3554, Train Recall: 0.3570, Train Precision: 0.3539, Val Loss: 0.7683, Val Accuracy: 0.7852, Val F1: 0.2133, Val Recall: 0.4202, Val Precision: 0.1429, Best Threshold: 0.59\n",
            "Epoch 10/200, Loss: 0.0324, Train Accuracy: 0.7863, Train F1: 0.3620, Train Recall: 0.3637, Train Precision: 0.3603, Val Loss: 0.7651, Val Accuracy: 0.7866, Val F1: 0.2117, Val Recall: 0.4135, Val Precision: 0.1423, Best Threshold: 0.59\n",
            "Epoch 11/200, Loss: 0.0323, Train Accuracy: 0.7796, Train F1: 0.3788, Train Recall: 0.4032, Train Precision: 0.3572, Val Loss: 0.7723, Val Accuracy: 0.7764, Val F1: 0.2109, Val Recall: 0.4312, Val Precision: 0.1396, Best Threshold: 0.59\n",
            "Epoch 12/200, Loss: 0.0321, Train Accuracy: 0.7806, Train F1: 0.3784, Train Recall: 0.4007, Train Precision: 0.3585, Val Loss: 0.7737, Val Accuracy: 0.7774, Val F1: 0.2105, Val Recall: 0.4283, Val Precision: 0.1396, Best Threshold: 0.59\n",
            "Epoch 13/200, Loss: 0.0320, Train Accuracy: 0.7870, Train F1: 0.3789, Train Recall: 0.3898, Train Precision: 0.3686, Val Loss: 0.7728, Val Accuracy: 0.7858, Val F1: 0.2098, Val Recall: 0.4105, Val Precision: 0.1409, Best Threshold: 0.59\n",
            "Epoch 14/200, Loss: 0.0313, Train Accuracy: 0.7861, Train F1: 0.3989, Train Recall: 0.4259, Train Precision: 0.3751, Val Loss: 0.7815, Val Accuracy: 0.7777, Val F1: 0.2094, Val Recall: 0.4249, Val Precision: 0.1389, Best Threshold: 0.59\n",
            "Epoch 15/200, Loss: 0.0310, Train Accuracy: 0.7860, Train F1: 0.4052, Train Recall: 0.4373, Train Precision: 0.3775, Val Loss: 0.7872, Val Accuracy: 0.7747, Val F1: 0.2077, Val Recall: 0.4262, Val Precision: 0.1373, Best Threshold: 0.59\n",
            "Epoch 16/200, Loss: 0.0307, Train Accuracy: 0.7846, Train F1: 0.4173, Train Recall: 0.4629, Train Precision: 0.3799, Val Loss: 0.7977, Val Accuracy: 0.7674, Val F1: 0.2073, Val Recall: 0.4388, Val Precision: 0.1357, Best Threshold: 0.59\n",
            "Epoch 17/200, Loss: 0.0306, Train Accuracy: 0.7940, Train F1: 0.4117, Train Recall: 0.4324, Train Precision: 0.3929, Val Loss: 0.7914, Val Accuracy: 0.7835, Val F1: 0.2074, Val Recall: 0.4088, Val Precision: 0.1390, Best Threshold: 0.59\n",
            "Early stopping triggered after 17 epochs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/My Drive/Colab Notebooks/neural_net_model.pth\"  # Path where your model was saved\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKwUx0FuVtlY",
        "outputId": "410f0759-0721-48c3-eae7-ef49572d9963"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-1ffd2c3f3191>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (fc1): Linear(in_features=28, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (relu2): ReLU()\n",
              "  (dropout1): Dropout(p=0.2, inplace=False)\n",
              "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess X_test (if not done earlier)\n",
        "X_test_num_scaled = scaler.transform(X_test[numer])  # Only scale the numerical features\n",
        "X_test_scaled1 = pd.DataFrame(X_test_num_scaled, columns=numer, index=X_test.index)\n",
        "X_test_scaled = pd.concat([X_test_scaled1, X_test[dummy]], axis=1)\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "ubpec1fYN0NM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor).squeeze()  # Raw logits\n",
        "\n",
        "# Convert logits to probabilities using sigmoid\n",
        "test_probabilities = torch.sigmoid(test_outputs).numpy()\n",
        "\n",
        "# Apply the best threshold for classification\n",
        "test_predictions = (test_probabilities > best_threshold).astype(int)"
      ],
      "metadata": {
        "id": "0NbhG-6SVqXP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = accuracy_score(y_test_tensor.numpy(), test_predictions)\n",
        "test_f1 = f1_score(y_test_tensor.numpy(), test_predictions)\n",
        "test_recall = recall_score(y_test_tensor.numpy(), test_predictions)\n",
        "test_precision = precision_score(y_test_tensor.numpy(), test_predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j53QkV4bVz45",
        "outputId": "aae76b8a-dc16-4c6c-db16-df3ba3180cb1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7874\n",
            "Test F1 Score: 0.2127\n",
            "Test Recall: 0.4143\n",
            "Test Precision: 0.1431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers_iqr(df, columns, factor=1.5):\n",
        "    df_cleaned = df.copy()  # Make a copy of the DataFrame to avoid modifying original data\n",
        "\n",
        "    for column in columns:\n",
        "        if column in df_cleaned.columns:\n",
        "            Q1 = df_cleaned[column].quantile(0.25)  # 25th percentile\n",
        "            Q3 = df_cleaned[column].quantile(0.75)  # 75th percentile\n",
        "            IQR = Q3 - Q1\n",
        "\n",
        "            lower_bound = Q1 - factor * IQR\n",
        "            upper_bound = Q3 + factor * IQR\n",
        "\n",
        "            # Filter rows within the IQR bounds\n",
        "            df_cleaned[column] = df_cleaned[column].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "        else:\n",
        "            print(f\"Warning: Column '{column}' not found in DataFrame.\")\n",
        "\n",
        "    return df_cleaned"
      ],
      "metadata": {
        "id": "NkNuXAOSJfCP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "guXxQakUiv-u"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/testing_loan_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij4QqfkSkTdZ",
        "outputId": "fa6c2eb0-09b6-4c25-ade6-c3fe7fb3fc12"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-57f9f7af733e>:1: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  test_data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/testing_loan_data.csv\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_test_data(test_data, scaler, numer):\n",
        "    \"\"\"\n",
        "    Preprocess the test dataset by performing cleaning, encoding, and scaling operations.\n",
        "\n",
        "    Parameters:\n",
        "    - test_data (pd.DataFrame): The raw test dataset.\n",
        "    - scaler (sklearn.preprocessing.StandardScaler): Fitted scaler for numerical feature scaling.\n",
        "    - numer (list): List of numerical column names.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Preprocessed and scaled test dataset.\n",
        "    \"\"\"\n",
        "    # Drop unnecessary columns\n",
        "    miscol = ['id', 'application_approved_flag', 'tot_hi_cred_lim', 'revol_util',\n",
        "              'loan_amnt', 'bc_util', 'desc', 'member_id', 'mths_since_last_major_derog']\n",
        "    test_data.drop(miscol, axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "    # Process 'term' column\n",
        "    test_data['term'] = test_data['term'].str.extract(r'(\\d+)').astype(float)\n",
        "\n",
        "    # Process 'emp_length' column\n",
        "    test_data['emp_length'] = test_data['emp_length'].str.extract(r'(\\d+)').astype(float)\n",
        "    test_data['emp_length'] = test_data['emp_length'].fillna(0)  # Assume missing values as 0 (less than a year)\n",
        "\n",
        "    # Process percentage columns\n",
        "    percentage_columns = ['int_rate']\n",
        "    for col in percentage_columns:\n",
        "        if col in test_data.columns and test_data[col].dtype == 'object':\n",
        "            test_data[col] = test_data[col].str.replace('%', '').astype(float) / 100\n",
        "\n",
        "    # Fill missing values\n",
        "    test_data['mths_since_recent_inq'] = test_data['mths_since_recent_inq'].fillna(0)\n",
        "\n",
        "    columns_to_impute = ['percent_bc_gt_75', 'total_bc_limit', 'tot_cur_bal']\n",
        "    for col in columns_to_impute:\n",
        "        test_data[col] = test_data[col].fillna(test_data[col].median())\n",
        "\n",
        "    # Handle categorical variables\n",
        "    test_data['home_ownership'] = test_data['home_ownership'].replace(['OTHER', 'NONE'], 'OTHER')\n",
        "    categorical_columns = ['purpose', 'home_ownership']\n",
        "    test_data_encoded = pd.get_dummies(test_data, columns=categorical_columns, drop_first=False, dtype=int)\n",
        "\n",
        "    # Remove outliers\n",
        "    columns_to_check = ['term', 'int_rate', 'emp_length', 'annual_inc', 'percent_bc_gt_75',\n",
        "                        'dti', 'inq_last_6mths', 'mths_since_recent_inq',\n",
        "                        'total_bc_limit', 'tot_cur_bal', 'internal_score']\n",
        "    cleaned_data = remove_outliers_iqr(test_data_encoded, columns_to_check)\n",
        "\n",
        "    # Scale numerical columns\n",
        "    dummy = cleaned_data.columns.difference(numer)\n",
        "    cleaned_data_num_scaled = scaler.transform(cleaned_data[numer])\n",
        "    cleaned_data_scaled1 = pd.DataFrame(cleaned_data_num_scaled, columns=numer, index=cleaned_data.index)\n",
        "    cleaned_data_scaled = pd.concat([cleaned_data_scaled1, cleaned_data[dummy]], axis=1)\n",
        "\n",
        "    return cleaned_data_scaled"
      ],
      "metadata": {
        "id": "ZQD0ZZjCw8j0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_test_data = preprocess_test_data(test_data, scaler, numer)"
      ],
      "metadata": {
        "id": "6bTXaMDnw_Xk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert test data to PyTorch tensors\n",
        "X_test_tensor = torch.tensor(processed_test_data.values, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader for batch processing (optional)\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor), batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "9hrRCvc5xYQR"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        inputs = inputs[0]  # Extract inputs from TensorDataset\n",
        "        outputs = model(inputs).squeeze()  # Forward pass\n",
        "        probabilities = torch.sigmoid(outputs)  # Convert logits to probabilities\n",
        "\n",
        "        # Apply the best threshold\n",
        "        predictions = (probabilities > 0.56).int()\n",
        "\n",
        "        all_predictions.extend(predictions.tolist())\n",
        "\n",
        "# ===========================\n",
        "# 4. Fill Target Column\n",
        "# ===========================\n",
        "# Insert predictions into the test dataset\n",
        "test_data[\"bad_flag\"] = all_predictions  # Replace 'target' with the actual column name\n",
        "\n",
        "# Save the updated test dataset\n",
        "# test_data.to_csv(\"/content/drive/My Drive/Colab Notebooks/test_predictions_filled.csv\", index=False)\n",
        "\n",
        "print(\"Predictions saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfrLzBrfxbMG",
        "outputId": "9c246c04-641a-489c-a470-5b5aaa8a982d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3zPzvumnjWtL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}